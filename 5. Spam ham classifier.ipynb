{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    }
   ],
   "source": [
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo',\n",
       " 'como',\n",
       " 'más',\n",
       " 'pero',\n",
       " 'sus',\n",
       " 'le',\n",
       " 'ya',\n",
       " 'o',\n",
       " 'este',\n",
       " 'sí',\n",
       " 'porque',\n",
       " 'esta',\n",
       " 'entre',\n",
       " 'cuando',\n",
       " 'muy',\n",
       " 'sin',\n",
       " 'sobre',\n",
       " 'también',\n",
       " 'me',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'donde',\n",
       " 'quien',\n",
       " 'desde',\n",
       " 'todo',\n",
       " 'nos',\n",
       " 'durante',\n",
       " 'todos',\n",
       " 'uno',\n",
       " 'les',\n",
       " 'ni',\n",
       " 'contra',\n",
       " 'otros',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'ante',\n",
       " 'ellos',\n",
       " 'e',\n",
       " 'esto',\n",
       " 'mí',\n",
       " 'antes',\n",
       " 'algunos',\n",
       " 'qué',\n",
       " 'unos',\n",
       " 'yo',\n",
       " 'otro',\n",
       " 'otras',\n",
       " 'otra',\n",
       " 'él',\n",
       " 'tanto',\n",
       " 'esa',\n",
       " 'estos',\n",
       " 'mucho',\n",
       " 'quienes',\n",
       " 'nada',\n",
       " 'muchos',\n",
       " 'cual',\n",
       " 'poco',\n",
       " 'ella',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'algunas',\n",
       " 'algo',\n",
       " 'nosotros',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'tú',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'ellas',\n",
       " 'nosotras',\n",
       " 'vosotros',\n",
       " 'vosotras',\n",
       " 'os',\n",
       " 'mío',\n",
       " 'mía',\n",
       " 'míos',\n",
       " 'mías',\n",
       " 'tuyo',\n",
       " 'tuya',\n",
       " 'tuyos',\n",
       " 'tuyas',\n",
       " 'suyo',\n",
       " 'suya',\n",
       " 'suyos',\n",
       " 'suyas',\n",
       " 'nuestro',\n",
       " 'nuestra',\n",
       " 'nuestros',\n",
       " 'nuestras',\n",
       " 'vuestro',\n",
       " 'vuestra',\n",
       " 'vuestros',\n",
       " 'vuestras',\n",
       " 'esos',\n",
       " 'esas',\n",
       " 'estoy',\n",
       " 'estás',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'esté',\n",
       " 'estés',\n",
       " 'estemos',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estaré',\n",
       " 'estarás',\n",
       " 'estará',\n",
       " 'estaremos',\n",
       " 'estaréis',\n",
       " 'estarán',\n",
       " 'estaría',\n",
       " 'estarías',\n",
       " 'estaríamos',\n",
       " 'estaríais',\n",
       " 'estarían',\n",
       " 'estaba',\n",
       " 'estabas',\n",
       " 'estábamos',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estuve',\n",
       " 'estuviste',\n",
       " 'estuvo',\n",
       " 'estuvimos',\n",
       " 'estuvisteis',\n",
       " 'estuvieron',\n",
       " 'estuviera',\n",
       " 'estuvieras',\n",
       " 'estuviéramos',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuviese',\n",
       " 'estuvieses',\n",
       " 'estuviésemos',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estando',\n",
       " 'estado',\n",
       " 'estada',\n",
       " 'estados',\n",
       " 'estadas',\n",
       " 'estad',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ha',\n",
       " 'hemos',\n",
       " 'habéis',\n",
       " 'han',\n",
       " 'haya',\n",
       " 'hayas',\n",
       " 'hayamos',\n",
       " 'hayáis',\n",
       " 'hayan',\n",
       " 'habré',\n",
       " 'habrás',\n",
       " 'habrá',\n",
       " 'habremos',\n",
       " 'habréis',\n",
       " 'habrán',\n",
       " 'habría',\n",
       " 'habrías',\n",
       " 'habríamos',\n",
       " 'habríais',\n",
       " 'habrían',\n",
       " 'había',\n",
       " 'habías',\n",
       " 'habíamos',\n",
       " 'habíais',\n",
       " 'habían',\n",
       " 'hube',\n",
       " 'hubiste',\n",
       " 'hubo',\n",
       " 'hubimos',\n",
       " 'hubisteis',\n",
       " 'hubieron',\n",
       " 'hubiera',\n",
       " 'hubieras',\n",
       " 'hubiéramos',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubiese',\n",
       " 'hubieses',\n",
       " 'hubiésemos',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'habiendo',\n",
       " 'habido',\n",
       " 'habida',\n",
       " 'habidos',\n",
       " 'habidas',\n",
       " 'soy',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'somos',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'seas',\n",
       " 'seamos',\n",
       " 'seáis',\n",
       " 'sean',\n",
       " 'seré',\n",
       " 'serás',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'seréis',\n",
       " 'serán',\n",
       " 'sería',\n",
       " 'serías',\n",
       " 'seríamos',\n",
       " 'seríais',\n",
       " 'serían',\n",
       " 'era',\n",
       " 'eras',\n",
       " 'éramos',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'fui',\n",
       " 'fuiste',\n",
       " 'fue',\n",
       " 'fuimos',\n",
       " 'fuisteis',\n",
       " 'fueron',\n",
       " 'fuera',\n",
       " 'fueras',\n",
       " 'fuéramos',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fuese',\n",
       " 'fueses',\n",
       " 'fuésemos',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'sintiendo',\n",
       " 'sentido',\n",
       " 'sentida',\n",
       " 'sentidos',\n",
       " 'sentidas',\n",
       " 'siente',\n",
       " 'sentid',\n",
       " 'tengo',\n",
       " 'tienes',\n",
       " 'tiene',\n",
       " 'tenemos',\n",
       " 'tenéis',\n",
       " 'tienen',\n",
       " 'tenga',\n",
       " 'tengas',\n",
       " 'tengamos',\n",
       " 'tengáis',\n",
       " 'tengan',\n",
       " 'tendré',\n",
       " 'tendrás',\n",
       " 'tendrá',\n",
       " 'tendremos',\n",
       " 'tendréis',\n",
       " 'tendrán',\n",
       " 'tendría',\n",
       " 'tendrías',\n",
       " 'tendríamos',\n",
       " 'tendríais',\n",
       " 'tendrían',\n",
       " 'tenía',\n",
       " 'tenías',\n",
       " 'teníamos',\n",
       " 'teníais',\n",
       " 'tenían',\n",
       " 'tuve',\n",
       " 'tuviste',\n",
       " 'tuvo',\n",
       " 'tuvimos',\n",
       " 'tuvisteis',\n",
       " 'tuvieron',\n",
       " 'tuviera',\n",
       " 'tuvieras',\n",
       " 'tuviéramos',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuviese',\n",
       " 'tuvieses',\n",
       " 'tuviésemos',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'teniendo',\n",
       " 'tenido',\n",
       " 'tenida',\n",
       " 'tenidos',\n",
       " 'tenidas',\n",
       " 'tened']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv('SMSSpamCollection',sep= '\\t', names= ['labels','message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                            message\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>ham</td>\n",
       "      <td>U sick still can go shopping?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm at home n ready...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sad story of a Man - Last week was my b'day. M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>ham</td>\n",
       "      <td>soon you will have the real thing princess! Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hi msg me:)i'm in office..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>ham</td>\n",
       "      <td>I sent you  &amp;lt;#&amp;gt;  bucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3508</th>\n",
       "      <td>ham</td>\n",
       "      <td>Two fundamentals of cool life: \"Walk, like you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5148</th>\n",
       "      <td>ham</td>\n",
       "      <td>K..then come wenever u lik to come and also te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lor wat time ü finish?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     labels                                            message\n",
       "4128    ham                             Sorry, I'll call later\n",
       "2835    ham                      U sick still can go shopping?\n",
       "4041    ham                             I'm at home n ready...\n",
       "2259    ham  Sad story of a Man - Last week was my b'day. M...\n",
       "2204    ham  soon you will have the real thing princess! Do...\n",
       "561     ham                         Hi msg me:)i'm in office..\n",
       "201     ham                       I sent you  &lt;#&gt;  bucks\n",
       "3508    ham  Two fundamentals of cool life: \"Walk, like you...\n",
       "5148    ham  K..then come wenever u lik to come and also te...\n",
       "2474    ham                          Ok lor wat time ü finish?"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       message                                                               \n",
       "         count unique                                                top freq\n",
       "labels                                                                       \n",
       "ham       4825   4516                             Sorry, I'll call later   30\n",
       "spam       747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('labels').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Count the number of words in each message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['length'] = messages['message'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                            message  length\n",
       "0    ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1    ham                      Ok lar... Joking wif u oni...      29\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3    ham  U dun say so early hor... U c already then say...      49\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages['length'].plot(kind= 'hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.489950\n",
       "std        59.942907\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[messages['length']==910]['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[messages['length']==2]['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess = 'Sample message! Hi, how are you? Notice the punctuation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "a\n",
      "m\n",
      "p\n",
      "l\n",
      "e\n",
      " \n",
      "m\n",
      "e\n",
      "s\n",
      "s\n",
      "a\n",
      "g\n",
      "e\n",
      "!\n",
      " \n",
      "H\n",
      "i\n",
      ",\n",
      " \n",
      "h\n",
      "o\n",
      "w\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      "?\n",
      " \n",
      "N\n",
      "o\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "p\n",
      "u\n",
      "n\n",
      "c\n",
      "t\n",
      "u\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "for char in mess:\n",
    "    print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "nopunc = [char for char in mess if char not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'a',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'm',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'a',\n",
       " 'g',\n",
       " 'e',\n",
       " ' ',\n",
       " 'H',\n",
       " 'i',\n",
       " ' ',\n",
       " 'h',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'y',\n",
       " 'o',\n",
       " 'u',\n",
       " ' ',\n",
       " 'N',\n",
       " 'o',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'u',\n",
       " 'n',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "nopunc = \"\".join(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample message Hi how are you Notice the punctuation'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate the polarity of the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets calculate the Polarity of the Reviews\n",
    "def get_polarity(text):\n",
    "    textblob = TextBlob(str(text))\n",
    "    pol = textblob.sentiment.polarity\n",
    "    return pol\n",
    "\n",
    "# lets apply the function\n",
    "messages['polarity'] =messages['message'].apply(get_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                            message  length  polarity\n",
       "0    ham  Go until jurong point, crazy.. Available only ...     111      0.15\n",
       "1    ham                      Ok lar... Joking wif u oni...      29      0.50\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...     155      0.30\n",
       "3    ham  U dun say so early hor... U c already then say...      49      0.10\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...      61      0.00"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Remove all the punctuation from the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# lets remove Punctuations from the Reviews\n",
    "def punctuation_removal(text):\n",
    "    clean_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(clean_list)\n",
    "    return clean_str\n",
    "\n",
    "messages['message'] = messages['message'].apply(punctuation_removal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Remove all the numbers from the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make a function to remove Numbers from the reviews \n",
    "import re\n",
    "def drop_number(text):\n",
    "    list_text_new = []\n",
    "    for i in text:\n",
    "        if not re.search('\\d', i): \n",
    "            list_text_new.append(i)\n",
    "    return ''.join(list_text_new)\n",
    "            \n",
    "messages['message'] = messages['message'].apply(drop_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert all the numbers from lowercase and lemmatise them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "#for lemmatisation\n",
    "\n",
    "def lemmatise(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    text_lemm = [lm.lemmatize(word.lower()) for word in text_tokens]\n",
    "    return \" \".join(text_lemm)\n",
    "\n",
    "messages['message'] = messages['message'].apply(lemmatise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       go until jurong point crazy available only in ...\n",
       "1                                 ok lar joking wif u oni\n",
       "2       free entry in a wkly comp to win fa cup final ...\n",
       "3             u dun say so early hor u c already then say\n",
       "4       nah i dont think he go to usf he life around h...\n",
       "                              ...                        \n",
       "5567    this is the nd time we have tried contact u u ...\n",
       "5568                  will ü b going to esplanade fr home\n",
       "5569      pity wa in mood for that soany other suggestion\n",
       "5570    the guy did some bitching but i acted like id ...\n",
       "5571                              rofl it true to it name\n",
       "Name: message, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Remove all the stopwords from the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for stopwords Removal\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens = [word for word in text_tokens if not word in stopwords.words('english')]\n",
    "    tokens_text = ' '.join(tokens)\n",
    "    return tokens_text\n",
    "\n",
    "messages['message'] = messages['message'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Convert textual data to numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer = CountVectorizer().fit(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7954"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_transformer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'please dont text anymore nothing else say'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = messages['message'][100]\n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg1 = 'medicine university' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4122)\t1\n",
      "  (0, 7222)\t1\n"
     ]
    }
   ],
   "source": [
    "bow1 = bow_transformer.transform([msg1])\n",
    "print(bow1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 284)\t1\n",
      "  (0, 1850)\t1\n",
      "  (0, 2020)\t1\n",
      "  (0, 4601)\t1\n",
      "  (0, 5085)\t1\n",
      "  (0, 5842)\t1\n",
      "  (0, 6789)\t1\n"
     ]
    }
   ],
   "source": [
    "bow3 = bow_transformer.transform([msg])\n",
    "print(bow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7954)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anymore'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_transformer.get_feature_names()[284]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dont'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_transformer.get_feature_names()[1850]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_bow = bow_transformer.transform(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 7954)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency\n",
    "# number of times the term appears / total terms in the document \n",
    "\n",
    "# Inverse Document Frequency\n",
    "# log_e(total documents / documents with that term in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidftransformer = TfidfTransformer().fit(message_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfTest = tfidftransformer.transform(bow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7222)\t0.7071067811865475\n",
      "  (0, 4122)\t0.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "tfidfTest1 = tfidftransformer.transform(bow1)\n",
    "print(tfidfTest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6789)\t0.2972515892817849\n",
      "  (0, 5842)\t0.33430339108513046\n",
      "  (0, 5085)\t0.3258516789845504\n",
      "  (0, 4601)\t0.4076415297305482\n",
      "  (0, 2020)\t0.4380455120892207\n",
      "  (0, 1850)\t0.2787254613687773\n",
      "  (0, 284)\t0.5078976751704797\n"
     ]
    }
   ],
   "source": [
    "print(tfidfTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.239394426449646"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidftransformer.idf_[bow_transformer.vocabulary_['university']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "messagesTfidf = tfidftransformer.transform(message_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 7954)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messagesTfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamDetectionModel = MultinomialNB().fit(messagesTfidf,messages['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamDetectionModel.predict(tfidfTest1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['labels'][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPrediction = spamDetectionModel.predict(messagesTfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'spam', ..., 'ham', 'ham', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.99      4825\n",
      "        spam       1.00      0.83      0.91       747\n",
      "\n",
      "    accuracy                           0.98      5572\n",
      "   macro avg       0.99      0.92      0.95      5572\n",
      "weighted avg       0.98      0.98      0.98      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(messages['labels'], allPrediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_Test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_train, msg_test, label_train, label_test = train_test_split(messages['message'], messages['labels'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457 1115 4457 1115\n"
     ]
    }
   ],
   "source": [
    "print(len(msg_train), len(msg_test), len(label_train), len(label_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using piplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('bow',CountVectorizer()),\n",
    "                     ('tfidf',TfidfTransformer()),\n",
    "                     ('classifier',MultinomialNB())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bow', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinePred = pipeline.predict(['this was all about Natural language processing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelinePred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.96      0.98      1016\n",
      "        spam       0.70      1.00      0.82        99\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.85      0.98      0.90      1115\n",
      "weighted avg       0.97      0.96      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pipelinePred, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
